# -*- coding: utf-8 -*-
"""Tarefa 1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10L2IkQ0oUgHkAitbahzlrtXrEFkDkCYi

# Rede Neural Simples

### Implementando uma RNA Simples

O diagrama abaixo mostra uma rede simples. A combinação linear dos pesos, inputs e viés formam o input h, que então é passado pela função de ativação f(h), gerando o output final do perceptron, etiquetado como y.
 <img src='RNA-simples.png' /><br>

<p style="text-align:center">  <i> Diagrama de uma rede neural simples</i> </p>
 

Círculos são unidades, caixas são operações. O que faz as redes neurais possíveis, é que a função de ativação, f(h) pode ser qualquer função, não apenas a função degrau.

<p> Por exemplo, caso f(h)=h, o output será o mesmo que o input. Agora o output da rede é </p>

<p style="text-align:center"> $$h = \frac 1n\sum_{i=1}^n(w_i*x_i)+b$$  </p>

<p> Essa equação deveria ser familiar para você, pois é a mesma do modelo de regressão linear!
Outras funções de ativação comuns são a função logística (também chamada de sigmóide), tanh e a função softmax. Nós iremos trabalhar principalmente com a função sigmóide pelo resto dessa aula:</p>


$$f(h) = sigmoid(h)=\frac 1 {1+e^{-h}}$$

## Vamos implementar uma RNA de apenas um neurônio!

#### Importando a biblioteca
"""

import numpy as np

"""#### Função do cáculo da sigmóide"""

def sigmoid(x):
    return 1/(1+np.exp(-x))

"""#### Vetor dos valores de entrada"""

x = np.array([0.2,-0.5])
b=0.1

"""#### Pesos das ligações sinápticas"""

w = np.array([0.6,-0.4])

"""#### Calcule a combinação linear de entradas e pesos sinápticos"""

h=np.dot(x,w)+b

"""#### Aplicado a função de ativação do neurônio"""

y=sigmoid(h)

print('A saída da rede é',y)



